{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a6d52b-9507-4487-8a26-2dc4e4bd8b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install cmdstanpy prophet lightgbm prefect holidays --quiet\n",
    "%pip install -U opentelemetry-api --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d03c21-c2be-4739-8489-14b0735e678f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import sys\n",
    "sys.path.append(\"/Workspace/Repos/desareca/santiago-weather-forecast\")\n",
    "\n",
    "from src.data.ingestion import load_from_delta_table\n",
    "from src.data.preprocessing import prepare_time_series, train_test_split_temporal\n",
    "from src.models.arima_model import ARIMAPredictor\n",
    "from src.models.prophet_model import ProphetPredictor\n",
    "from src.models.lightgbm_model import LightGBMPredictor\n",
    "from src.evaluation.metrics import plot_predictions\n",
    "from src.utils.config import *\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "print(\"‚úÖ Setup completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053ab45e-b278-455e-bfbf-75fca0a88633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CARGANDO MEJORES MODELOS DESDE MLFLOW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Obtener experimento\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "print(f\"\\nüìä Experimento: {EXPERIMENT_NAME}\")\n",
    "print(f\"   ID: {experiment.experiment_id}\")\n",
    "\n",
    "# Filtrar solo parent runs (excluir folds individuales)\n",
    "all_runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"tags.run_type = 'parent' and tags.evaluation_type = 'cross_validation'\",\n",
    "    order_by=[\"metrics.cv_avg_rmse ASC\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nüìà Total configuraciones de modelos encontradas: {len(all_runs)}\")\n",
    "\n",
    "if len(all_runs) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No se encontraron runs de experimentaci√≥n con CV.\")\n",
    "    print(\"   Ejecuta primero el notebook 03_model_experiments con grid search.\")\n",
    "else:\n",
    "    print(\"\\nüîù Top 10 configuraciones por F1-Score (CV):\")\n",
    "    \n",
    "    # Construir datos para mostrar\n",
    "    display_data = []\n",
    "    \n",
    "    for idx, row in all_runs.head(10).iterrows():\n",
    "        model_family = row.get('tags.model_family', 'Unknown')\n",
    "        \n",
    "        # Construir string de hiperpar√°metros seg√∫n familia\n",
    "        if model_family == 'ARIMA':\n",
    "            p = row.get('params.p', 'N/A')\n",
    "            d = row.get('params.d', 'N/A')\n",
    "            q = row.get('params.q', 'N/A')\n",
    "            hyperparams = f\"({p},{d},{q})\"\n",
    "            \n",
    "        elif model_family == 'Prophet':\n",
    "            cp = row.get('params.changepoint_prior_scale', 'N/A')\n",
    "            ss = row.get('params.seasonality_prior_scale', 'N/A')\n",
    "            hyperparams = f\"cp={cp}, ss={ss}\"\n",
    "                    \n",
    "        elif model_family == 'LightGBM':\n",
    "            n_est = row.get('params.n_estimators', 'N/A')\n",
    "            lr = row.get('params.learning_rate', 'N/A')\n",
    "            depth = row.get('params.max_depth', 'N/A')\n",
    "            hyperparams = f\"trees={n_est}, lr={lr}, d={depth}\"\n",
    "        else:\n",
    "            hyperparams = \"N/A\"\n",
    "        \n",
    "        display_data.append({\n",
    "            'Rank': idx + 1,\n",
    "            'Familia': model_family,\n",
    "            'Hiperpar√°metros': hyperparams,\n",
    "            'F1 (CV)': f\"{row.get('metrics.cv_avg_f1_score', 0):.3f}\",\n",
    "            'RMSE (CV)': f\"{row.get('metrics.cv_avg_rmse', 0):.2f}\",\n",
    "            'Descripci√≥n': row.get('tags.description', 'N/A')[:40]\n",
    "        })\n",
    "    \n",
    "    df_display = pd.DataFrame(display_data)\n",
    "    print(df_display.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8555ad95-f91a-4e7a-abdf-0839b7071fcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SELECCI√ìN DE MEJORES MODELOS POR FAMILIA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Filtrar mejores por familia (solo parent runs)\n",
    "best_arima = all_runs[all_runs['tags.model_family'] == 'ARIMA'].head(1)\n",
    "best_prophet = all_runs[all_runs['tags.model_family'] == 'Prophet'].head(1)\n",
    "best_lgbm = all_runs[all_runs['tags.model_family'] == 'LightGBM'].head(1)\n",
    "\n",
    "# Funci√≥n para mostrar info detallada\n",
    "def print_model_info(df, family_name):\n",
    "    if len(df) == 0:\n",
    "        print(f\"\\n‚ùå No se encontr√≥ modelo {family_name}\")\n",
    "        return None\n",
    "    \n",
    "    row = df.iloc[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ü•á Mejor {family_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"   Run ID: {row['run_id'][:12]}...\")\n",
    "    print(f\"   F1-Score (CV): {row.get('metrics.cv_avg_f1_score', 0):.3f}\")\n",
    "    print(f\"   MAE (CV): {row.get('metrics.cv_avg_mae', 0):.3f} mm\")\n",
    "    print(f\"   RMSE (CV): {row.get('metrics.cv_avg_rmse', 0):.3f} mm\")\n",
    "    \n",
    "    # Mostrar hiperpar√°metros espec√≠ficos\n",
    "    print(f\"\\n   üìã Hiperpar√°metros:\")\n",
    "    \n",
    "    if family_name == 'ARIMA':\n",
    "        print(f\"      p = {row.get('params.p', 'N/A')}\")\n",
    "        print(f\"      d = {row.get('params.d', 'N/A')}\")\n",
    "        print(f\"      q = {row.get('params.q', 'N/A')}\")\n",
    "        \n",
    "    elif family_name == 'Prophet':\n",
    "        print(f\"      yearly_seasonality = {row.get('params.yearly_seasonality', 'N/A')}\")\n",
    "        print(f\"      weekly_seasonality = {row.get('params.weekly_seasonality', 'N/A')}\")\n",
    "        print(f\"      changepoint_prior_scale = {row.get('params.changepoint_prior_scale', 'N/A')}\")\n",
    "        print(f\"      seasonality_prior_scale = {row.get('params.seasonality_prior_scale', 'N/A')}\")\n",
    "            \n",
    "    elif family_name == 'LightGBM':\n",
    "        print(f\"      n_estimators = {row.get('params.n_estimators', 'N/A')}\")\n",
    "        print(f\"      learning_rate = {row.get('params.learning_rate', 'N/A')}\")\n",
    "        print(f\"      max_depth = {row.get('params.max_depth', 'N/A')}\")\n",
    "        print(f\"      num_leaves = {row.get('params.num_leaves', 'N/A')}\")\n",
    "        print(f\"      lags = {row.get('params.lags', 'N/A')}\")\n",
    "        print(f\"      rolling_windows = {row.get('params.rolling_windows', 'N/A')}\")\n",
    "    \n",
    "    if 'tags.description' in row.index and pd.notna(row['tags.description']):\n",
    "        print(f\"\\n   üìù Descripci√≥n: {row['tags.description']}\")\n",
    "    \n",
    "    return row\n",
    "\n",
    "arima_info = print_model_info(best_arima, \"ARIMA\")\n",
    "prophet_info = print_model_info(best_prophet, \"Prophet\")\n",
    "lgbm_info = print_model_info(best_lgbm, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63a0095-9eee-4990-b446-23c43bcbbddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CARGAR MODELOS ENTRENADOS DESDE MLFLOW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models_to_test = []\n",
    "\n",
    "# Funci√≥n para cargar modelo desde MLflow\n",
    "def load_trained_model(run_id, model_family, params):\n",
    "    \"\"\"Carga modelo entrenado desde MLflow y lo envuelve en su predictor\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Cargar modelo de MLflow\n",
    "        model_uri = f\"runs:/{run_id}/model\"\n",
    "        loaded_sklearn_model = mlflow.sklearn.load_model(model_uri)\n",
    "        \n",
    "        # Crear wrapper del predictor\n",
    "        if model_family == 'ARIMA':\n",
    "            wrapper = ARIMAPredictor(\n",
    "                p=params['p'],\n",
    "                d=params['d'],\n",
    "                q=params['q']\n",
    "            )\n",
    "        elif model_family == 'Prophet':\n",
    "            wrapper = ProphetPredictor(\n",
    "                yearly_seasonality=params['yearly_seasonality'],\n",
    "                weekly_seasonality=params['weekly_seasonality'],\n",
    "                daily_seasonality=params['daily_seasonality'],\n",
    "                changepoint_prior_scale=params['changepoint_prior_scale'],\n",
    "                seasonality_prior_scale=params['seasonality_prior_scale']\n",
    "            )\n",
    "        elif model_family == 'LightGBM':\n",
    "            wrapper = LightGBMPredictor(\n",
    "                n_estimators=params['n_estimators'],\n",
    "                learning_rate=params['learning_rate'],\n",
    "                max_depth=params['max_depth'],\n",
    "                num_leaves=params['num_leaves'],\n",
    "                min_child_samples=params['min_child_samples'],\n",
    "                subsample=params['subsample'],\n",
    "                colsample_bytree=params['colsample_bytree'],\n",
    "                reg_alpha=params['reg_alpha'],\n",
    "                reg_lambda=params['reg_lambda'],\n",
    "                lags=params['lags'],\n",
    "                rolling_windows=params['rolling_windows']\n",
    "            )\n",
    "        \n",
    "        # Asignar el modelo cargado al wrapper\n",
    "        wrapper.model = loaded_sklearn_model\n",
    "        \n",
    "        # Para LightGBM, necesitamos tambi√©n el train_history\n",
    "        if model_family == 'LightGBM':\n",
    "            # Se asignar√° cuando hagamos fit con train\n",
    "            wrapper.train_history = None\n",
    "        \n",
    "        return wrapper\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando modelo: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Cargar ARIMA\n",
    "if arima_info is not None:\n",
    "    print(\"\\nüîÑ Cargando ARIMA desde MLflow...\")\n",
    "    \n",
    "    arima_params = {\n",
    "        'p': int(arima_info.get('params.p', 1)),\n",
    "        'd': int(arima_info.get('params.d', 1)),\n",
    "        'q': int(arima_info.get('params.q', 1))\n",
    "    }\n",
    "    \n",
    "    arima_wrapper = load_trained_model(arima_info['run_id'], 'ARIMA', arima_params)\n",
    "    \n",
    "    if arima_wrapper:\n",
    "        models_to_test.append({\n",
    "            'name': 'ARIMA',\n",
    "            'model': arima_wrapper,\n",
    "            'params': arima_params,\n",
    "            'cv_rmse': arima_info.get('metrics.cv_avg_rmse', 0)\n",
    "        })\n",
    "        print(f\"‚úÖ ARIMA cargado desde run {arima_info['run_id'][:8]}...\")\n",
    "\n",
    "\n",
    "# Cargar Prophet\n",
    "if prophet_info is not None:\n",
    "    print(\"\\nüîÑ Cargando Prophet desde MLflow...\")\n",
    "    \n",
    "    prophet_params = {\n",
    "        'yearly_seasonality': prophet_info.get('params.yearly_seasonality', 'True') == 'True',\n",
    "        'weekly_seasonality': prophet_info.get('params.weekly_seasonality', 'False') == 'True',\n",
    "        'daily_seasonality': prophet_info.get('params.daily_seasonality', 'False') == 'True',\n",
    "        'changepoint_prior_scale': float(prophet_info.get('params.changepoint_prior_scale', 0.05)),\n",
    "        'seasonality_prior_scale': float(prophet_info.get('params.seasonality_prior_scale', 10.0))\n",
    "    }\n",
    "    \n",
    "    prophet_wrapper = load_trained_model(prophet_info['run_id'], 'Prophet', prophet_params)\n",
    "    \n",
    "    if prophet_wrapper:\n",
    "        models_to_test.append({\n",
    "            'name': 'Prophet',\n",
    "            'model': prophet_wrapper,\n",
    "            'params': prophet_params,\n",
    "            'cv_rmse': prophet_info.get('metrics.cv_avg_rmse', 0)\n",
    "        })\n",
    "        print(f\"‚úÖ Prophet cargado desde run {prophet_info['run_id'][:8]}...\")\n",
    "\n",
    "\n",
    "# Cargar LightGBM\n",
    "if lgbm_info is not None:\n",
    "    print(\"\\nüîÑ Cargando LightGBM desde MLflow...\")\n",
    "    \n",
    "    lags_str = lgbm_info.get('params.lags', '[1, 7, 30]')\n",
    "    rolling_str = lgbm_info.get('params.rolling_windows', '[7, 30]')\n",
    "    \n",
    "    lgbm_params = {\n",
    "        'n_estimators': int(lgbm_info.get('params.n_estimators', 100)),\n",
    "        'learning_rate': float(lgbm_info.get('params.learning_rate', 0.1)),\n",
    "        'max_depth': int(lgbm_info.get('params.max_depth', 5)),\n",
    "        'num_leaves': int(lgbm_info.get('params.num_leaves', 31)),\n",
    "        'min_child_samples': int(lgbm_info.get('params.min_child_samples', 20)),\n",
    "        'subsample': float(lgbm_info.get('params.subsample', 0.8)),\n",
    "        'colsample_bytree': float(lgbm_info.get('params.colsample_bytree', 0.8)),\n",
    "        'reg_alpha': float(lgbm_info.get('params.reg_alpha', 0.0)),\n",
    "        'reg_lambda': float(lgbm_info.get('params.reg_lambda', 0.0)),\n",
    "        'lags': eval(lags_str) if isinstance(lags_str, str) else [1, 7, 30],\n",
    "        'rolling_windows': eval(rolling_str) if isinstance(rolling_str, str) else [7, 30]\n",
    "    }\n",
    "    \n",
    "    lgbm_wrapper = load_trained_model(lgbm_info['run_id'], 'LightGBM', lgbm_params)\n",
    "    \n",
    "    if lgbm_wrapper:\n",
    "        models_to_test.append({\n",
    "            'name': 'LightGBM',\n",
    "            'model': lgbm_wrapper,\n",
    "            'params': lgbm_params,\n",
    "            'cv_rmse': lgbm_info.get('metrics.cv_avg_rmse', 0)\n",
    "        })\n",
    "        print(f\"‚úÖ LightGBM cargado desde run {lgbm_info['run_id'][:8]}...\")\n",
    "\n",
    "\n",
    "print(f\"\\nüìã Total modelos cargados exitosamente: {len(models_to_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7d22a0-ff18-489a-9ed7-28fc9bd00f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPARANDO DATOS PARA EVALUACI√ìN EN TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar datos\n",
    "df = load_from_delta_table(\"weather_raw\", spark)\n",
    "serie = prepare_time_series(df, target_col=\"precipitacion\")\n",
    "train, test = train_test_split_temporal(serie, train_ratio=TRAIN_SPLIT)\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   Total: {len(serie)} d√≠as\")\n",
    "print(f\"   Train: {len(train)} d√≠as ({train.index.min().date()} ‚Üí {train.index.max().date()})\")\n",
    "print(f\"   Test: {len(test)} d√≠as ({test.index.min().date()} ‚Üí {test.index.max().date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c193a75-721d-4631-900c-decef0b720b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUACI√ìN EN TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_test = []\n",
    "predictions_dict = {}\n",
    "\n",
    "for i, model_config in enumerate(models_to_test):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{i+1}/{len(models_to_test)}] Evaluando {model_config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model = model_config['model']\n",
    "    \n",
    "    # El modelo est√° entrenado con dataset completo, pero necesitamos\n",
    "    # re-entrenar solo con train para evaluar correctamente en test\n",
    "    print(f\"\\nüöÄ Reentrenando {model_config['name']} con datos de train...\")\n",
    "    model.fit(train)\n",
    "    \n",
    "    # Predecir\n",
    "    print(f\"üîÆ Generando predicciones...\")\n",
    "    preds = model.predict(steps=len(test))\n",
    "    preds.index = test.index\n",
    "    predictions_dict[model_config['name']] = preds\n",
    "    \n",
    "    # Evaluar\n",
    "    print(f\"üìä Evaluando...\")\n",
    "    metrics = model.evaluate(test, preds)\n",
    "    \n",
    "    # Agregar a resultados\n",
    "    results_test.append({\n",
    "        'Modelo': model_config['name'],\n",
    "        'RMSE (CV)': model_config['cv_rmse'],\n",
    "        'RMSE (Test)': metrics['rmse'],\n",
    "        'MAE (Test)': metrics['mae'],\n",
    "        'R¬≤ (Test)': metrics['r2'],\n",
    "        'F1 (Test)': metrics['f1_score'],\n",
    "        'Accuracy (Test)': metrics['accuracy'],\n",
    "        'Precision (Test)': metrics['precision'],\n",
    "        'Recall (Test)': metrics['recall']\n",
    "    })\n",
    "\n",
    "# Crear DataFrame de resultados\n",
    "df_results = pd.DataFrame(results_test)\n",
    "df_results = df_results.sort_values('RMSE (Test)', ascending=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS FINALES EN TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3619d30f-1f20-4dbf-9997-0ee621a48dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AN√ÅLISIS DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Comparar RMSE CV vs Test\n",
    "print(\"\\nüìä Comparaci√≥n RMSE: CV vs Test:\")\n",
    "for _, row in df_results.iterrows():\n",
    "    diff = row['RMSE (Test)'] - row['RMSE (CV)']\n",
    "    symbol = \"üìà\" if diff > 0 else \"üìâ\"\n",
    "    print(f\"   {symbol} {row['Modelo']:10s}: CV={row['RMSE (CV)']:.3f} ‚Üí Test={row['RMSE (Test)']:.3f} (Œî={diff:+.3f})\")\n",
    "\n",
    "# Ganadores por m√©trica\n",
    "print(\"\\nüèÜ Ganadores por m√©trica:\")\n",
    "metrics_winner = {\n",
    "    'RMSE (Test)': ('min', df_results),\n",
    "    'MAE (Test)': ('min', df_results),\n",
    "    'R¬≤ (Test)': ('max', df_results),\n",
    "    'F1 (Test)': ('max', df_results),\n",
    "    'Accuracy (Test)': ('max', df_results)\n",
    "}\n",
    "\n",
    "for metric, (direction, df) in metrics_winner.items():\n",
    "    if direction == 'min':\n",
    "        winner = df.loc[df[metric].idxmin(), 'Modelo']\n",
    "        value = df[metric].min()\n",
    "    else:\n",
    "        winner = df.loc[df[metric].idxmax(), 'Modelo']\n",
    "        value = df[metric].max()\n",
    "    print(f\"   {metric:20s}: {winner:10s} ({value:.3f})\")\n",
    "\n",
    "# Mejor modelo\n",
    "best_model = df_results.iloc[0]\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üèÜ MEJOR MODELO EN TEST SET (menor RMSE): {best_model['Modelo']}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"   RMSE: {best_model['RMSE (Test)']:.3f} mm\")\n",
    "print(f\"   MAE:  {best_model['MAE (Test)']:.3f} mm\")\n",
    "print(f\"   R¬≤:   {best_model['R¬≤ (Test)']:.3f}\")\n",
    "print(f\"   F1:   {best_model['F1 (Test)']:.3f}\")\n",
    "print(f\"   Accuracy: {best_model['Accuracy (Test)']*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüí° Interpretaci√≥n de resultados:\")\n",
    "print(f\"   - ARIMA: Predice casi siempre 0mm (sin lluvia) ‚Üí Alta accuracy pero F1=0\")\n",
    "print(f\"   - Prophet: Predice lluvia muy frecuentemente ‚Üí Recall=100% pero baja precision\")\n",
    "print(f\"   - LightGBM: Balance entre ambos extremos ‚Üí Mejor F1 general\")\n",
    "print(f\"   - R¬≤ negativo: Los modelos no superan un predictor naive (media)\")\n",
    "print(f\"   - Santiago tiene ~70% d√≠as secos ‚Üí Problema de desbalanceo severo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "668b0aff-0693-42b7-b9ee-ee0d1310afb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZACI√ìN DE PREDICCIONES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Colores por modelo\n",
    "colors = {\n",
    "    'ARIMA': 'steelblue',\n",
    "    'Prophet': 'coral',\n",
    "    'LightGBM': 'green'\n",
    "}\n",
    "\n",
    "# Plot real\n",
    "ax.plot(test.index, test.values,\n",
    "        label='Real', color='black',\n",
    "        alpha=0.8, linewidth=2)\n",
    "\n",
    "# Plot predicciones\n",
    "for model_name, preds in predictions_dict.items():\n",
    "    model_metrics = df_results[df_results['Modelo'] == model_name].iloc[0]\n",
    "    \n",
    "    ax.plot(preds.index, preds.values,\n",
    "            label=f\"{model_name} (RMSE={model_metrics['RMSE (Test)']:.2f}, F1={model_metrics['F1 (Test)']:.3f})\",\n",
    "            color=colors[model_name],\n",
    "            alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax.set_title('Predicciones vs Real - Test Set', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Fecha', fontsize=11)\n",
    "ax.set_ylabel('Precipitaci√≥n (mm)', fontsize=11)\n",
    "ax.legend(fontsize=10, loc='upper right')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9430594b-597d-4bfd-84c6-c398dcf1b47e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MATRIZ DE CONFUSI√ìN (Clasificaci√≥n Lluvia S√≠/No)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "threshold = 1.0  # mm\n",
    "n_models = len(predictions_dict)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
    "\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, preds) in enumerate(predictions_dict.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Convertir a clasificaci√≥n binaria\n",
    "    y_true_binary = (test > threshold).astype(int)\n",
    "    y_pred_binary = (preds > threshold).astype(int)\n",
    "    \n",
    "    # Calcular matriz de confusi√≥n\n",
    "    tp = ((y_true_binary == 1) & (y_pred_binary == 1)).sum()\n",
    "    fp = ((y_true_binary == 0) & (y_pred_binary == 1)).sum()\n",
    "    tn = ((y_true_binary == 0) & (y_pred_binary == 0)).sum()\n",
    "    fn = ((y_true_binary == 1) & (y_pred_binary == 0)).sum()\n",
    "    \n",
    "    conf_matrix = pd.DataFrame(\n",
    "        [[tn, fp], [fn, tp]],\n",
    "        columns=['Pred: No Lluvia', 'Pred: Lluvia'],\n",
    "        index=['Real: No Lluvia', 'Real: Lluvia']\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', \n",
    "                cmap='Blues', ax=ax, cbar=False)\n",
    "    \n",
    "    model_metrics = df_results[df_results['Modelo'] == model_name].iloc[0]\n",
    "    ax.set_title(\n",
    "        f\"{model_name}\\nPrecision={model_metrics['Precision (Test)']*100:.1f}%, \"\n",
    "        f\"Recall={model_metrics['Recall (Test)']*100:.1f}%\",\n",
    "        fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ea2fdc-3456-4a22-a470-7944b0370eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REGISTRAR MEJOR MODELO EN MLFLOW REGISTRY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Seleccionar mejor modelo por RMSE\n",
    "best_model_name = df_results.iloc[0]['Modelo']\n",
    "best_model_config = next(m for m in models_to_test if m['name'] == best_model_name)\n",
    "\n",
    "print(f\"\\nüèÜ Registrando: {best_model_name}\")\n",
    "print(f\"   RMSE (Test): {df_results.iloc[0]['RMSE (Test)']:.3f} mm\")\n",
    "print(f\"   MAE (Test):  {df_results.iloc[0]['MAE (Test)']:.3f} mm\")\n",
    "print(f\"   F1 (Test):   {df_results.iloc[0]['F1 (Test)']:.3f}\")\n",
    "\n",
    "# Reentrenar con serie completa\n",
    "print(f\"\\nüöÄ Reentrenando {best_model_name} con serie completa ({len(serie)} d√≠as)...\")\n",
    "best_model_config['model'].fit(serie)\n",
    "print(f\"‚úÖ Modelo final entrenado\")\n",
    "\n",
    "# Registrar en MLflow\n",
    "with mlflow.start_run(run_name=f\"{best_model_name}_PRODUCTION_CANDIDATE\"):\n",
    "    # Tags\n",
    "    mlflow.set_tag(\"run_type\", \"production_candidate\")\n",
    "    mlflow.set_tag(\"model_family\", best_model_name)\n",
    "    mlflow.set_tag(\"run_stage\", \"production\")\n",
    "    mlflow.set_tag(\"description\", f\"Mejor modelo en test set - {best_model_name}\")\n",
    "    \n",
    "    # Par√°metros\n",
    "    mlflow.log_params(best_model_config['params'])\n",
    "    mlflow.log_param(\"model_type\", best_model_name)\n",
    "    mlflow.log_param(\"trained_on\", \"full_dataset\")\n",
    "    mlflow.log_param(\"n_samples\", len(serie))\n",
    "    mlflow.log_param(\"date_range\", f\"{serie.index.min().date()} to {serie.index.max().date()}\")\n",
    "    \n",
    "    # M√©tricas de test\n",
    "    mlflow.log_metric(\"test_rmse\", df_results.iloc[0]['RMSE (Test)'])\n",
    "    mlflow.log_metric(\"test_mae\", df_results.iloc[0]['MAE (Test)'])\n",
    "    mlflow.log_metric(\"test_r2\", df_results.iloc[0]['R¬≤ (Test)'])\n",
    "    mlflow.log_metric(\"test_f1\", df_results.iloc[0]['F1 (Test)'])\n",
    "    mlflow.log_metric(\"test_accuracy\", df_results.iloc[0]['Accuracy (Test)'])\n",
    "    \n",
    "    # M√©trica CV de referencia\n",
    "    mlflow.log_metric(\"cv_rmse\", best_model_config['cv_rmse'])\n",
    "    \n",
    "    # Guardar modelo\n",
    "    mlflow.sklearn.log_model(best_model_config['model'].model, \"model\")\n",
    "    \n",
    "    # Registrar en Model Registry\n",
    "    model_uri = f\"runs:/{mlflow.active_run().info.run_id}/model\"\n",
    "    registered_model = mlflow.register_model(model_uri, \"santiago_weather_predictor\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ MODELO REGISTRADO EXITOSAMENTE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\n  üìù Nombre: {registered_model.name}\")\n",
    "    print(f\"  üî¢ Versi√≥n: {registered_model.version}\")\n",
    "    print(f\"  üè∑Ô∏è  Tipo: {best_model_name}\")\n",
    "    print(f\"  üìâ RMSE (Test): {df_results.iloc[0]['RMSE (Test)']:.3f} mm\")\n",
    "    print(f\"  üìä F1 (Test): {df_results.iloc[0]['F1 (Test)']:.3f}\")\n",
    "    print(f\"\\n  üîó URI: {model_uri}\")\n",
    "    print(f\"\\n  ‚û°Ô∏è  Ir a MLflow UI ‚Üí Models para ver el modelo registrado\")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_model_evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
